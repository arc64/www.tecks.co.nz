---
has_children: false
layout: default
nav_order: 4
title: 'Problems inherent in the hypothetico-deductive method practised by social
  scientists '
---
# Problems inherent in the hypothetico-deductive method practised by social scientists 


::: documentByline
Prepared by John Church, PhD, School of Educational Studies and Human
Development

University of Canterbury, Christchurch, New Zealand.
:::

::: {#parent-fieldname-text-16aae81b183e416a956cba5a7799f85d}
One of the most common uses of the between-groups experiment is to test
experimenter-generated hypotheses about relationships between
independent and dependent variables. Hypothesis testing has an important
role to play in the development of scientific understanding. However, in
order for hypothesis testing to work - in order for it to generate
useful results - certain conditions must be met.

First, it must be possible to get either a confirmatory or
disconfirmatory result and this is possible only if there is a close
agreement between the theoretical concepts and the empirical measures of
these concepts. This is rarely the case in a between-groups experiment.
Most cognitive science constructs refer to large, poorly defined classes
of behaviour. Whenever an experiment fails to turn out as expected, the
results can be argued away by arguing that the outcome measures used
"were not valid measures of *x*"*.*

Secondly, as Popper (1963) has argued, experimental results which fail
to confirm a particular hypothesis are far more informative than
experimental results which confirm a particular hypothesis. This is
because an hypothesis can never be demonstrated to be true no matter how
many times it has been confirmed while a single disconfirmation may
demonstrate the hypothesis not to be true. So the primary aim of those
who seek to pursue the hypothetico-deductive method should be to conduct
experiments which subject current hypotheses to the most critical test
possible and the most valuable experimental results should be those
which fail to confirm a currently accepted hypothesis.

The most critical test is, of course, a test of the most specific
hypothesis. An example might be that instructional procedure *y* results
in a rate of acquisition which is 2.5 times greater than that produced
by a standard procedure. (This is much more specific - much riskier -
that the standard NHST hypothesis that procedure *y* results a "faster"
rate of acquisition.) It is the failure to reject risky hypotheses which
provides the strongest corroboration for a theory.

However, many between-groups experiments are so poorly designed that a
non-significant result usually fails to provide convincing evidence that
the tested hypothesis is false. This is because "there are numerous
different theoretical ways that could account for some correlation or
difference being nonzero . . . a theory tested in this way takes only a
small risk of being refuted if it is false" (Meehl, 1997, p. 408).

Furthermore, the editors of social science journals routinely favour
reports of significant results over reports of non-significant results
and have done so for more than half a century (Bakan, 1967). One of the
effects of this practice is to suppress the results of experiments which
fail to achieve statistical significance, regardless of whether this
failure is due to a non-existent effect, a weak effect, a poorly
controlled experiment, or a poorly designed measure of learning. In
fact, the results of experiments where the null hypothesis could not be
rejected are often not even submitted for publication because the
authors know in advance that such experiments tend to be rejected by
journal editors. This leads to the file drawer effect which so often
biases attempts to produce a meta-analysis of previous research in a
particular field. The effect of this routine suppression of
non-confirmatory results over a long period of time is difficult to
determine but may well have resulted in the survival of many hypotheses
long past their use-by date. Furthermore, it has almost certainly
removed one of the main incentives which might have operated to produce
improvements in experimental control and experimental method, that is,
the publication of conflicting results.

Thirdly, the interpretation of experimental data is a complex process
which requires the application of good judgement. The experimenter must
consider all rival hypotheses including the possibility that the
dependent variable measures may not have been up to standard, the groups
may have been incomparable from the outset, the experiment may have been
too short, the treatments may have been inadequately controlled, and so
on. It is also possible that a non-significant result may simply
indicate that we have not yet discovered the variables which need to be
controlled (Sidman, 1960). An over reliance on statistical significance
testing too often results in a failure to take into account "the many
details of experimental method or extraneous factors that might bear on
the conclusions" (Johnston & Pennypacker, 1993, p. 186). "Despite the
awesome pre-eminence this method has attained in our experimental
journals and textbooks . . . it is based upon a fundamental
misunderstanding of the nature of rational inference, and is seldom if
ever appropriate to the aims of scientific research" (Rozeboom, 1960, p.
417).

Proponents of statistical significance testing would do well to abandon
it, at least to the extent of abandoning the corrupt scientific method.
If they must do statistical significance testing, they should do it
after the results have been interpreted with respect to the research
hypothesis as the scientific method requires. There are always rival
hypotheses to consider and the null hypothesis should rightfully take
its place among these secondarily important hypotheses (Carver, 1978, p.
394).

By substituting statistics for replication, and a statistical criterion
for the reasoned interpretation of empirical data, an edifice has been
erected which could be collapsed *simply by changing the criterion*.
This is because "there is absolutely no reason (at least provided by the
method) why the point of statistical \'significance\' should be set at
the 95% level, rather than, say the 94% or 96% level" (Rozeboom, 1960,
p. 417). Research findings which have been accepted could be rejected,
not as a result of subsequent experiments failing to replicate the
original experiment, but *simply as a result of a change* in the rule
which is being applied in interpreting the results. The adoption of the
95% confidence interval *as a convention* means that

The body of knowledge constituting much of contemporary psychology could
be overturned not by the introduction of *new* evidence, but by a simple
change in procedure. By shifting the usual and convenient level of
significance from 5 per cent to 3 per cent or 1 per cent, much of what
currently constitutes the body of knowledge of experimental psychology .
. . would change according to the new level of significance. . . .
Furthermore, reliance on the results of significance tests as a form of
evidence for or against scientific assertions brings into question the
meaning of the term \'evidence\' when that evidence may be reversed from
one level of confidence to another. At one level of confidence, a result
may favor a scientific assertion, while at another level the same result
may go against the same assertion (Chiesa, 1994, p. 79).

Generally speaking, the journals which publish the results of social
science and cognitive science experiments do not require any
demonstration by the experimenters that their results are reproducible,
that is, replication is not a requirement for publication. As a
consequence, the great majority of social science experiments in
teaching and learning have never been replicated and the reliability
(reproducibility) of their results remains completely unknown.
:::

::: referencesList
#### References

-   Bakan, D. (1967). On method: Toward a reconstruction of
    psychological investigation. San Francisco: Jossey-Bass.
-   Carver, R. P. (1978). The case against statistical significance
    testing. Harvard Educational Review, 48, 378-399.
-   Chiesa, M. (1994). Radical behaviorism: The philosophy and the
    science. Boston: Authors Cooperative, Inc.
-   Johnston, J. M., & Pennypacker, H. S. (1993). Readings for
    strategies and tactics of behavioral research (2nd ed.). Hillsdale,
    NJ: Lawrence Erlbaum Associates.
-   Meehl, P. E. (1997). The problem is epistemology, not statistics:
    Replace significance tests by confidence intervals and quantify
    accuracy of risky numerical predictions. In L. L. Harlow, S. A.
    Mulaik, & J. H. Steiger (Eds.), What if there were no significance
    tests? (pp. 394-425). Mahwah, N.J.: Lawrence Erlbaum Associates.
-   Popper, K. (1963). Conjectures and refutations: The growth of
    scientific knowledge. London: Routledge and Kegan Paul.
-   Rozeboom, W. W. (1960). The fallacy of the null-hypothesis
    significance test. Psychological Bulletin, 57, 416-428.
-   Sidman, M. (1960). Tactics of scientific research: Evaluating
    experimental data in psychology. New York: Basic Books.
:::
