---
has_children: false
layout: default
nav_order: 0
title: 'Evidence gathered using what kinds of research? '
---
# Evidence gathered using what kinds of research? 


::: documentByline
Prepared by John Church, PhD, School of Educational Studies and Human
Development

University of Canterbury, Christchurch, New Zealand.
:::

::: {#parent-fieldname-text-8443f2cad9694e33bf33e603499bf3be}
It is possible to make the case that teaching practice should be based
on the results of research, but only if it is understood that questions
about teaching can take a variety of forms. Sometimes we are interested
in descriptive questions such as "What is happening here?" Sometimes we
are interested in questions about whether or not something has an
effect. For example, "Does this teaching practice have a reproducible
effect on learning?" And sometimes we are interested in explanation. For
example "How (or why) does this practice have this effect on children's
motivation?" (Feuer, Towne & Shavelson, 2002). Attempts to answer these
different kinds of questions require the application of different kinds
of research procedures.

**Different kinds of research methods**

Put in a slightly different way, educational researchers use a variety
of different kinds of research methods and these different methods
produce quite different kinds of information (Keeves, 1997). Some
research procedures collect only descriptive information. Much of the
research into classroom practice, for example, consists of ethnographic
and other kinds of descriptive research. (For an example, see Angier and
Povey (1999)). It is not possible, however, to distinguish between
teaching practices which are more or less effective in bringing about
learning by simply describing what teachers are currently doing or by
describing the way in which teachers justify what they are currently
doing. To identify cause and effect relationships between elements of
teaching practice and student learning, it is necessary to conduct
experimental analyses of those teaching practices. Claims about
particular types of teaching practices and the effect which they have on
particular types of learning cannot be warranted (supported) by
reference to the results of descriptive studies no matter how elegantly
these are carried out.

Some research procedures collect information on the relationship between
one or more teaching variables and one or more learning outcomes in
naturally occurring situations but present only data describing the
correlations between subsets of these variables. (For an example, see
Nuthall and Alton-Lee (1993)). While correlational studies can suggest
that a relationship may exist between teaching practice X and learning
outcome Y, only an experiment can demonstrate that the learning outcome
in question is a direct function of X and not a function of something
else and only a series of experiments can demonstrate that X is a
necessary condition for the development of Y in certain types of
learners.

Different kinds of educational experiments are also possible. Some
experimental evaluations employ randomised groups designs of one kind or
another to measure the average effect of a particular teaching practice
or teaching programme on the mean level of post-instructional
achievement of a group of learners relative to that of a control group
of closely similar learners who did not experience the instruction being
evaluated. (For an example, see Chun and Winter (1999)).

The results of this kind of experiment, while useful for programme
evaluations, cannot be used to measure the effects of a particular
teaching practice on the learning of individual students. That is, this
method cannot answer the question which is of primary interest to
teachers "What shall I do for this student?" For this reason it cannot
be considered the "gold standard" in experimental research as some
researchers (e.g., Shavelson & Towne, 2002) have argued. In order to
measure the effects of any aspect of teaching on the learning of
individual students, an experimental method which preserves the
observations of the progress of each individual learner (rather than
averaging these) is required. Clinical trials must be preceded by
numerous experimental studies designed to identify which outcomes in
which children are affected by which interventions. "The randomized
experiment becomes a powerful tool for warranting causal effects after a
rather protracted process has identified the most promising
interventions for changing the most important outcomes for target
children in settings of interest" (Raudenbush, 2005, p. 29).

The experimental procedure which preserves the record of learning in
individual children and which has proved to be most productive is the
behaviour analysis experiment (the single case experiment). These
experiments follow the progress of individual students in the days
leading up to the experimental treatment, the days while it operated and
the days following. (For an example, see Umbriet, Lane and Dejud
(2004)). Unlike the results of a clinical trial which cannot be
generalised to individual learners, collections of single case
experiments can both identify cause and effect relationships between
teaching and learning and can be generalised to individual learners.

**Different kinds of review methods**

Similar variability exists across the scholarly reviews of research on
teaching. Some research reviews consist largely of anecdotes drawn from
ethnographic and other kinds of descriptive studies of classroom life
(e.g., Anthony & Walshaw, 2007). Because the individual studies say
nothing about the effects of particular teaching practices on particular
learning outcomes, few if any conclusions about teaching effects, or the
conditions necessary for learning, can be drawn from such reviews.

Some reviews of research on teaching take the form of a meta-analytic
review of a set of randomised groups experiments which meet certain
inclusion criteria with respect to the teaching process or procedure
under examination (e.g., National Reading Panel, 2000). While
conclusions regarding the effects of particular teaching strategies on
particular learning outcomes at the group or population level can
sometimes be drawn from such reviews, no conclusions regarding the
effects of a particular teaching strategy on the learning of individual
children can be drawn from this type of review. Furthermore, the results
of this type of review are frequently uninterpretable. This is the case
whenever

-   the children in different experiments varied with respect to age,
    developmental level or symptom severity but these groupings were not
    separately analysed
-   individual experiments used different outcome measures but no
    breakdown by outcome measure was undertaken
-   teaching interventions in the various experiments were of varying
    length but this is ignored in the review
-   the interventions (even although there were given the same name)
    were actually different
-   the review contains experiments with no-treatment controls and
    experiments with business-as-usual controls but these are not
    analysed separately.

Some reviews of research on teaching take the form of "best evidence
syntheses". Most of the recent New Zealand reviews of teaching take the
form of BES reviews. These reviews are of variable quality. Some (e.g.,
Timperley, Wilson, Barrar & Fung, 2007) provide a clear description of
the inclusion criteria employed and others (e.g., Alton Lee, 2003) do
not. The latter review includes descriptive studies, correlational
studies and experimental studies and treats them as if they are
producing comparable forms of knowledge which, of course, is not the
case. This kind of review generates few conclusions of any consequence
because the believability of individual conclusions is impossible to
determine.

Some reviews of research on teaching take the form of reviews of sets of
single case experiments. (For an example, see Cates (2005)). Generally
speaking these reviews apply inclusion criteria which are sufficiently
stringent to ensure that only well controlled experimental analyses of a
single teaching variable or a single learning outcome are included in
the review. Unlike most of the narrative reviews, meta-analyses, and
"best evidence syntheses", reviews of single case experimental analyses
of the effects of particular instructional variables on particular
learning outcomes often generate reliable and generalisable conclusions.

It can be seen that different kinds of research procedure generate
different kinds of knowledge. In order to make sense of the research
base on particular aspects of teaching practice a fairly sophisticated
understanding of the various types of research method and the kinds of
knowledge which they generate is required. A more detailed account of
these difficulties will be found in Book 3 of this website.
:::

::: referencesList
#### References

-   Alton-Lee, A. (2003). Quality teaching for diverse students in
    schooling: Best evidence synthesis. Wellington, New Zealand:
    Ministry of Education.
-   Angier, C. & Povey, H. (1999). One teacher and a class of school
    students: Their perception of the culture of their mathematics
    classroom and its construction. Educational Review, 51, 147-160.
-   Anthony, G. & Walshaw, M. (2007). Effective pedagogy in
    mathematics/PÃ¤ngarau. Best evidence synthesis Iteration. Wellington,
    New Zealand: Ministry of Education. Retrieved 16 June 2007 from
    http://educationcounts.edcentere.govt.nz/goto/BES.
-   Cates, G. L. (2005). A review of the effects of interspersing
    procedures on the stages of academic skill development. Journal of
    Behavioral Education, 14, 305-325.
-   Chun, C. C., & Winter, S. (1999). Classwide peer tutoring with or
    without reinforcement: Effects on academic responding, content
    coverage, achievement, intrinsic interest and reported project
    experiences. Educational Psychology, 19, 191-205.
-   Feuer, M. J., Towne, L., & Shavelson, R. J. (2002). Scientific
    culture and educational research. Educational Researcher, 31, 4-14.
-   Keeves, J. P. (Ed.) (1997). Educational research, methodology, and
    measurement: An international handbook (2nd ed.). Oxford, England:
    Pergamon/Elsevier Science Inc.
-   National Reading Panel. (2000). Teaching children to read. Retrieved
    June 25, 2000, from
    http://www.nichd.nih.gov/publications/nrp/smallbook.htm
-   Nuthall, G., & Alton-Lee, A. (1993). Predicting learning from
    student experience of teaching: A theory of student knowledge
    construction in classrooms. American Educational Research Journal,
    30, 799-840.
-   Raudenbush, S. R. (2005). Learning from attempts to improve
    schooling: The contribution of methodological diversity. Educational
    Researcher, 35(5), 25-31.
-   Shavelson, R. J., & Towne, L. (Eds.). (2002). Committee on
    scientific principles for educational research. Washington, DC:
    National Research Council, National Academy Press.
-   Timperley, H., Wilson, A., Barrar, H., & Fung, I. (2007). Teacher
    professional learning and development. Best evidence synthesis
    iteration (BES). Wellington, New Zealand: New Zealand Ministry of
    Education.
-   Umbreit, J., Lane, K. L., & Dejud, C. (2004). Improving classroom
    behavior by modifying task difficulty: Effects of increasing the
    difficulty of too-easy tasks. Journal of Positive Behavior
    Interventions, 6, 13-20.
:::
