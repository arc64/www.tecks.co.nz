# Evidence of what quality? \n

::: documentByline
Prepared by John Church, PhD, School of Educational Studies and Human
Development

University of Canterbury, Christchurch, New Zealand.
:::

::: {#parent-fieldname-text-34d24f50b7fc4edb82789114f7c699cd}
Research in education tends to be of very variable quality -- ranging
from poorly controlled "action research" studies through to tightly
controlled single case experiments and randomised groups experiments.
Because different kinds of research questions can only be answered using
different kinds of research methods, there is no single set of quality
standards which can be applied to all research into learning and
teaching.

**Quality standards in the measurement of learning and behaviour
change**

Teaching researchers employ a wide variety of data collection procedures
ranging from hearsay reports through to video recordings of teacher and
student performance (Keeves, 1997). These data collection procedures
vary widely with respect to reliability and the data collected varies
widely with respect to accuracy. Reliability refers to the extent to
which an observation procedure can be operated in a consistent or
reproducible fashion*.* Accuracy refers to the closeness of the match
between the changes (learning) which actually occurred and the data
describing those changes (Johnston & Pennypacker, 1993).

Some observation and recording procedures are more reliable and produce
more accurate data than others. For example, recording by trained
observers is more reliable than recording by untrained observers,
recording by outside observers is more reliable than recording by the
participants themselves, and simple coding schemes with well defined
categories can be applied more reliably than complex coding schemes
(Cooper, Heron & Heward, 1987; Kazdin, 1977; Reid, 1970). Generally
speaking, low inference recording procedures can be operated more
reliably than high inference recording procedures.

We also know that video recording produces more accurate data than
recording by human observers, direct observation produces more accurate
data than indirect observation (e.g. rating scales and verbal reports),
observers who are blind to experimental conditions tend to produce more
accurate records than observers who are aware of experimental conditions
(Johnston & Pennypacker, 1993; Kazdin, 1977) and tests which measure
performance fluency on specific academic tasks provide a more accurate
measure of learning than most tests of achievement.

Because low inference recording and measurement procedures produce data
which is more trustworthy than high inference recording procedures,
research which uses low inference recording and measurement procedures
is more likely to meet "scientific" standards (in the sense of producing
potentially reproducible results) than research which uses high
inference recording and measurement procedures.

Because the data collected by teaching researchers varies widely with
respect to quality, making sense of the research base relevant to
teaching practice requires a fairly sophisticated understanding of the
relative accuracy of the data generated by the various observation,
testing and recording procedures currently in use. A more detailed
exploration of these difficulties will be found in Book 3 of this
website.

**Quality standards in design experiments**

One of the most important activities in research into teaching practice
is the design and formative evaluation of new teaching techniques,
procedures, programmes, and curricula. These activities are often
referred to as *design experiments.* The new programmes may be designed
by practitioners using the best available practitioner knowledge or they
may be developed using an iterative processes of controlled research
(often conducted under laboratory conditions) designed to identify those
conditions which are necessary conditions for mastery of the skills and
understandings being taught. We will refer to this latter process as
*scientific* *development*.

The scientific approach to instructional design was first described by
Markle and Tiemann (Markle & Tiemann, 1967; Tieman & Markle, 1991). It
is an adaptation of the Markle and Tiemann process which was employed by
Layng, Stikeleather and Twyman (2004) to develop the highly effective
Headsprout reading programme. The Headsprout development process
provides us with a model of the scientific approach to programme
development and evaluation against which all attempts at instructional
design can be compared. The process described by Layng at al. (2004) is
a process involving the following sequence of steps.

1\. *Analyse the content.* The content to be taught is examined and
classified according to the type of learning outcome involved (e.g.,
strategy, principle, concept, equivalence relation, operation, or motor
response).

2\. *State objectives.* The aims of the teaching programme are stated by
listing what the student is to be able to do at the end of the
instructional programme or instructional sequence.

3.*Design the criterion tests.* Tests are developed for each teaching
activity or routine within a lesson segment, for each lesson, for blocks
of lessons, and finally for the program. Tests of both accuracy
(understanding and retention) and fluency (automaticity) are employed.

4.*Identify the required entry skills.* Given what is to be learned,
determine the skills needed to progress through the program. Entry
repertoires are the specific prerequisites skills needed for success
(not just a list of prior experiences).

5\. *Build the instructional sequence*. The content analysis and the
criterion tests are used as a guide to the design of instructional
experiences and activities that will result in acquisition of the
desired criterion skills and understandings.

6\. *Formative evaluation.* The individual performance data collected
over successive field tests is used to iteratively refine the teaching
procedures until they meet the objectives listed in (2) as measured by
the criterion tests in (3). In this approach the behaviour of individual
learners shapes the program until nearly all learners meet the specified
criteria. For example, in the Headsprout Early Reading programme, "a
single learner makes about 200 meaningful responses per 20 minute
lesson, about 10 per minute. That means approximately 16,000 responses
are individually collected and analyzed during the course of the
program. As of this writing, across all learners, well over 100 million
instructional interactions . . . have been collected and regularly
examined in an effort to understand how learners interact with the
program and to continually improve it" (Layng et al., 2004, p. 6).

7.*Build in the motivating consequences*. Motivational elements usually
include a mixture of intrinsic elements (e.g., a high success rate on
practice responses) and extrinsic elements (e.g., interesting
activities, charts on which to record progress, and/or rewards for
achieving particular goals).

8.*Summative evaluation*. Once the programme has been refined to the
point where almost all individual children are achieving criterion
levels of performance, the programme as a whole is subjected to one or
more randomized control group studies so that its overall effectiveness
when compared against other programmes is known.

With this approach to instructional design, all elements of the program
are tested for effectiveness, and if particular criteria are not met,
alternative elements are designed and tested until they are. Evaluation
during the development phase is based on the performance of individuals
so that teachers who adopt the programme will be able to predict not
only group performance but also the individual performance of children
completing the programme (Twyman, Layng, Stikeleather & Hobbins, 2005).

**Quality standards in evaluation research**

Regardless of whether a teaching variable, practice or programme is
selected intuitively or as a result of scientific development its
effects on learning must still be measured. When the measurement of
instructional effects is undertaken in a controlled fashion it may be
referred to as *scientific evaluation* (to distinguish it from
scientific *development*).

In certain cases, a teaching practice or programme may qualify as
evidence-based when it has been evaluated using several well controlled
clinical trials where its effectiveness has been measured against that
of an existing practice or programme using appropriate samples of
learners who have been randomly assigned to the two teaching programmes.
However, there are many other ways of measuring the effects of teaching
on learning and any quality standards that are developed for evaluation
research must be applicable to the range of experimental designs which
can be used to measure instructional effects (Raudenbush, 2005).

In an attempt to clarify the kinds of evidence which should taken into
account in deciding whether a given practice is evidence-based or not,
Gina Green (1996) has argued that the believability of the research
evidence for a particular effect depends upon the extent to which
learning is observed directly (rather than indirectly), the level of
experimental control achieved, the extent to which the effects of
different ways of producing the same learning outcome can be directly
compared, and the extent to which particular research findings have been
replicated by different research teams.

Green provides the following diagram to summarise the main dimensions of
what we commonly refer to as research "method" and to locate various
commonly used research methods along a continuum from those which
provide the most to the least ambiguous evidence.

![Figure 1135 Types of evidence about treatment effects (from Green,
1996, p. 25)
](../../../../../../assets/images/figure-1135-types-of-evidence-about-treatment-effects-from-green-1996-p.-25 "Figure 1135 Types of evidence about treatment effects (from Green, 1996, p. 25) "){.image-inline
.captioned}

We have expanded Green's list into a set of *inclusion criteria*.
Inclusion criteria are the criteria which are to be applied when
attempting to decide whether the report of a particular investigation is
to be included for consideration in a research review. The inclusion
criteria which have been applied in the reviews appearing on this site
have been chosen on the grounds that investigations which meet these
inclusion criteria are more likely to produce reliable estimates of the
relative effects of different kinds of pedagogical practices on the rate
of learning of different kinds of skills and understandings in different
kinds of learners than those which do not. The inclusion criteria which
have been used in this site are as follows.

1.The investigation addressed a specific question about the effects of
some defined condition or aspect of teaching or teaching materials or
instructional programme on some type of student learning.

2.The investigation addressed a question the answer to which (or the
generality of the answer to which) was unknown at the time when the
research was undertaken.

3.The investigation employed a research method which, if appropriately
applied, could have answered the question which was asked.

4.The investigation employed a measure of learning which provided a
record of repeated observations of the learning demonstrated by each of
the individual learners who took part in the experiment.

5.The investigation demonstrated that an accurate measure of behaviour
change (learning) was obtained.

6.The investigation employed a research procedure which, when
appropriately applied, is known to produce an accurate measure of the
effect of a change in conditions on rate or degree of behaviour change
(learning).

7.The investigation demonstrated much the same effect across several
direct replications or the result has subsequently been replicated.

8.The generalisability of the results (if any) was clearly indicated or
is now being explored in subsequent investigations.

9.The report of the investigation described the research procedures,
research results, and theoretical explanations using standard terms with
agreed or defined meanings.

There are many aspects of teaching and learning which have yet to be
studied in any detail. This means that a particular aspect of learning
and/or teaching practice may be one for which there are no experimental
analyses which meet all of these criteria. At this point in the
development of evidence-based practice, it is necessary, therefore, to
allow inclusion of the occasional research report which does not meet
all of the above criteria -- simply because there is no research at all
which meets all of the criteria. Where the results of such
investigations are included, reviewers will be required to ensure that
the criteria which are and are not met by the investigation are given so
that the reader can assess the believability of the results of that
investigation.

In the interests of economy, we will refer to individual investigations
which meet these nine criteria as *scientific* and we will refer to
collections of scientific analyses of the effects of a particular
teaching practice as the *scientific research* into that practice.

We recognise that there is an ongoing debate amongst educational
researchers regarding what is to qualify as scientific research in the
study of learning and teaching (e.g., Berliner, 2002; Dixon & Carnine,
1994; Eisenhart, 2005; Erickson, 2005; Feuer, Towne & Shavelson, 2002;
Gee, 2005; Lather, 2004; Mayer, 2000; National Research Council, 2002;
Phillips, 2006; Raudenbush, 2005; Yates, 2005). This debate has been
going on for 80 years and is likely to continue until either the
activities which we have referred to as "scientific" are shown to
generate the understandings and the improvements in educational practice
which have been predicted or they do not.

However, there is a bottom line in this debate. Regardless of one's
personal view about the possibility of scientific research into learning
and teaching, the bottom line is that "we need principles that rule
things in and out of research, and we need principles of quality that
distinguish weak from strong research, depending upon the research
question and the research design" (Eisenhart, 2005, p. 57). This is why
increasing numbers of researchers are beginning to argue that "only when
the profession embraces scientific methods for determining efficacy and
accepts accountability for results will education acquire the status --
and the rewards -- of a mature profession (Carnine, 2000, p. 10).
:::

::: referencesList
#### References

-   Berliner, D. C. (2002). Educational research: The hardest science of
    all. Educational Researcher, 31(8), 18-21.
-   Carnine, D. (2000). Why education experts resist effective practices
    (And what it would take to make education more like medicine).
    Retrieved 10 December, 2000, from
    http://www.edexcellence.net/library/carnine.html
-   Cooper, J. O., Heron, T. E., & Heward, W. L. (1987). Applied
    behavior analysis. New York: Macmillan Publishing Co.
-   Dixon, R. & Carnine, D. (1994). Ideologies, practices, and their
    implications for special education. The Journal of Special
    Education, 28, 356-367.
-   Eisenhart, M. (2005). Science plus: A response to the responses to
    Scientific Research in Education. Teachers College Record, 107,
    52-58.
-   Erickson, F. (2005). Arts, humanities, and sciences in educational
    research and social engineering in federal education policy.
    Teachers College Record, 107, 4-9.
-   Feuer, M. J., Towne, L., & Shavelson, R. J. (2002). Scientific
    culture and educational research. Educational Researcher, 31(8),
    4-14.
-   Gee, J. P. (2005). It's theories all the way down: A response to
    Scientific Research in Education. Teachers College Record, 107,
    10-18.
-   Green, G. (1996). Evaluating claims about treatments for autism.
    In C. Maurice, G. Green, & S. L. Luce (Eds.), Behavioral
    intervention for young children with autism: A manual for parents
    and professionals (pp. 15-28). Austin: Pro-ed.
-   Grossen, B. (1996) What does it mean to be a research-based
    profession? Retrieved 10 April, 2005, from
    http://darkwing.uoregon.edu/\~bgrossen/pubs/resprf.htm#Science
-   Johnston, J. M., & Pennypacker, H. S. (1993). Strategies and tactics
    of behavioral research (2nd ed.). Hillsdale, NJ: Lawrence Erlbaum
    Associates.
-   Kazdin, A. E. (1977). Artefact, bias, and complexity of assessment:
    The ABCs of reliability. Journal of Applied Behavior Analysis, 10,
    141-150.
-   Keeves, J. P. (Ed.) (1997). Educational research, methodology, and
    measurement: An international handbook (2nd ed.). Oxford, England:
    Pergamon/Elsevier Science Inc.
-   Lather, P. (2004). Scientific research in education: A critical
    perspective. British Educational Research Journal, 30, 759-772.
-   Layng, T. V. J., Stikeleather, G., & Twyman, J. S. (2004).
    Scientific formative evaluation: The role of individual learners in
    generating and predicting successful educational outcomes. Retrieved
    15 July 2007 from
    http://headsprout.com/home/research_publications.cfm
-   Markle, S. M., & Tiemann, P. W. (1967). Programming is a process.
    Slide tape programme. Chicago: University of Illinois at Chicago.
-   Mayer, R. E. (2000). What is the place of science in educational
    research? Educational Researcher, 29(6), 38-39.
-   National Research Council. (2002). Scientific research in
    education. R. J. Shavelson & L. Towne (Eds.), Committee on
    Scientific Principles for Educational Research. Washington, DC:
    National Academy Press.
-   Phillips, D. C. (2006). Muddying the waters: The many purposes of
    educational inquiry. In C. F. Conrad & R. C. Serlin (Eds.), The Sage
    handbook for research in education: Engaging ideas and enriching
    inquiry (pp. 7-21). Thousand Oaks, CA: Sage Publications.
-   Raudenbush, S. R. (2005). Learning from attempts to improve
    schooling: The contribution of methodological diversity. Educational
    Researcher, 35(5), 25-31.
-   Reid, J. B. (1970). Reliability assessment of observation data: A
    possible methodological problem. Child Development, 41, 1143-1150.
-   Tieman, P. W., & Markle, S. M. (1991). Analysing instructional
    content. Champaign, IL: Stipes.
-   Twyman, J. S., Layng, T. V. J., Stikeleather, G., & Hobbins, K. A.
    (2005). A nonlinear approach to curriculum design: The role of
    behavior analysis in building an effective reading program. In W. L.
    Heward, T. E. Heron, N. A. Neef, S. M. Peterson, D. M. Sainato, G.
    Cartledge, et al. (Eds.), Focus on behavior analysis in education:
    Achievements, challenges, and opportunities. Upper Saddle River, NJ:
    Pearson Education.
-   Yates, G. C. R. (2005). "How obvious": Personal reflections on the
    database of educational psychology and effective teaching research.
    Educational Psychology, 25, 681-700.
:::
