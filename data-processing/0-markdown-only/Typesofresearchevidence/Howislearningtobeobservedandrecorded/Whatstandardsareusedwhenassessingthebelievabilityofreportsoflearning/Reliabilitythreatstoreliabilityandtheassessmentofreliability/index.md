# Reliability, threats to reliability and the assessment of reliability \n

::: documentByline
Prepared by John Church, PhD, School of Educational Studies and Human
Development

University of Canterbury, Christchurch, New Zealand.
:::

::: {#parent-fieldname-text-61c6cf172aae40bf96b8a7a4f0fb19c2}
"If the information produced by observing is to be of any long-term or
general use, it has to be repeatable information; in other words, it has
to be information that would be obtained by any one else attempting to
make the same observations" (Saslow, 1982, p. 10). The first question
which may be asked of the data collected during the course of a research
investigation is the question "Did the observation procedure operate in
a consistent fashion from one occasion to the next?"

**Definition.** In educational research, the extent to which an
observation or measurement procedure has been operated in a consistent
fashion is referred to as its *reliability.* (Some qualitative
researchers use the term *dependability*.) A test which generates the
same score when it is completed by the same individual on several
occasions (without any intervening learning) is referred to as a
reliable test and a coding procedure which generates the same count of
behaviour when it is applied to the same videotape of learner
performance on several separate occasions is said to be a reliable
coding procedure (Johnston & Pennypacker, 1993; Kerlinger, 1964).

**Threats to measurement reliability.** There are many factors which can
operate (during the course of a series of observations) to compromise
the reliability of a measurement procedure. Observer reliability may be
compromised by coding schemes which are too complex, by coding
categories which are inadequately defined, by failing to provide
adequate training for observers, by failing to provide observer
reliability checks at regular intervals, by fatigue or loss of
motivation on the part of observers, and so on.

Generally speaking, recording by highly trained observers is more
reliable than recording by poorly trained or untrained observers,
observers operate more consistently if they know that their accuracy is
going to be regularly checked than if no accuracy checks are
implemented, recording by outside observers is more reliable than
recording by the participants themselves, simple coding schemes with
well defined categories can be applied more reliably than complex coding
schemes or coding schemes with poorly defined categories, and so on
(Cooper, Heron & Heward, 1987; Johnson & Bolstad, 1973; Kazdin, 1977;
Reid, 1970).

Test reliability may also be compromised in many ways: by tests which
are too difficult, by ambiguously worded test questions, by test items
which allow guessing, by tests which are too long, by inadequate test
administration instructions, by changes in the testing conditions, by
changes in motivation on the part of those completing the test, by
inadequate marking guides, by failure to provide adequate training for
test markers, and so on.

Generally speaking, tests are more likely to function reliably if they
consist of items which test a single skill or a small collection of well
defined skills, use a number of items to assess each specific skill or
understanding, consist of well written questions and items which
discourage guessing, and are administered and marked by well trained
personnel who have been provided with clear test administration
guidelines and clear marking instructions (Allen & Yen, 1979; Popham,
1981).

**Assessing and reporting measurement reliability.** In order to assess
the reliability of a data collection procedure, that procedure must be
applied *to the same events* on more than one occasion and the degree of
identity between the two separate records assessed. This means that the
reliability of an observational (or coding) procedure using human
observers (or coders) can only be assessed if a sample of the behaviours
of interest can be preserved in some way - as a written product, audio
recording, or video recording, for example. Assessing the reliability of
a test is a relatively simply matter. The test is simply administered
twice to the same subjects taking care to ensure that no learning or
practice of the tested skills and understandings occurs during the two
administrations. (This is most commonly referred to as the *test-retest*
procedure for assessing test reliability.)

The basic measure of recorder reliability, coder reliability, and
test-retest reliability is the degree of correspondence or identity
between independently obtained records (or codes) of the same behaviour,
test performance, or self-report. The measures of correspondence which
are most commonly calculated and reported are either the percentage of
inter-occasion agreement or else the degree of inter-occasion
correlation. Both of these measures are measures of the extent to which
the two records of the same performance match each other.

The reader\'s reaction to differing levels of reported inter-occasion
agreement depends to some extent on the complexity of the coding scheme
which is being applied. If inter-marker agreement was being calculated
for the marking and check marking of written answers to a set of
mathematics exercises then anything less than 100% agreement between the
two markers would probably be regarded as unreliable. If the reliability
of a direct observation procedure was being assessed by getting two
observers each to independently code a videotaped sample of classroom
interactions using a complex 15 category code, then something better
than 80 per cent agreement between the observers might be accepted by
readers as demonstrating an adequate degree of coder reliability (Barlow
& Hersen, 1984).

*A note on percentage of agreement.* One common procedure which is used
to assess the "reliability" of the observations made by an observer (or
the classifications made by a coder) is to arrange for two separate
observers (or coders) to make parallel and independent records of the
same events. The researcher then calculates the degree of inter-observer
agreement (or inter-coder agreement) between the two independently
obtained records. Strictly speaking this procedure does not provide a
measure of the reliability of either of the observers (because only one
record of the same events is obtained from each observer) and it does
not provide a measure of the accuracy of the data obtained by the
primary observer (because the accuracy of the second observer is also
unknown). As a measure of data quality, measures of interobserver
agreement are simply that - measures of interobserver agreement. They
provide some information regarding data quality because it is unlikely
that high degrees of agreement could be obtained unless both observers
were operating in a reliable (consistent) fashion. Interobserver
agreement data provide important information regarding data quality in
situations where neither reliability nor accuracy can be assessed but a
measure of interobserver agreement is not a measure of either
reliability or accuracy (Johnston & Pennypacker, 1993).

*A note on the split-half method of calculating test "reliability".* In
order to calculate the reliability of a test, it is necessary to
administer the same test to the same learner(s) on more than one
occasion and to compare the scores obtained by each learner on each of
the several administrations of the test. Often however, a less onerous
procedure is used. The test is administered to a sample of learners, the
items in the test are divided into two halves (e.g. odd numbered items
and even numbered items), and the degree of correlation between the
scores on the two sets of items is calculated. Strictly speaking this
procedure does not provide a measure of test reliability because each
learner completes each item only once.
:::

::: referencesList
#### References

-   Allen, M. J., & Yen, W. M. (1979). Introduction to measurement
    theory. Belmont, CA: Wadsworth.
-   Barlow, D. H., & Hersen, M. (1984). Single case experimental
    designs: Strategies for studying behavior change (2nd ed.). New
    York: Pergamon.
-   Cooper, J. O., Heron, T. E., & Heward, W. L. (1987). Applied
    behavior analysis. New York: Macmillan Publishing Co.
-   Johnson, S. M., & Bolstad, O. D. (1973). Methodological issues in
    naturalistic observation: Some problems and solutions for field
    research. In L. A. Hamerlynck, L. C. Handy, & E. J. Mash (Eds.),
    Behavior change: Methodology, concepts and practice (pp. 7-67).
    Champaign, IL: Research Press.
-   Kazdin, A. E. (1977). Artefact, bias, and complexity of assessment:
    the ABCs of reliability. Journal of Applied Behavior Analysis, 10,
    141-150.
-   Kerlinger, F. N. (1964). Foundations of behavioral research:
    Educational and psychological inquiry. New York: Holt, Rinehart and
    Winston Inc.
-   Johnston, J. M., & Pennypacker, H. S. (1993). Strategies and tactics
    of behavioral research (2nd ed.). Hillsdale, NJ: Lawrence Erlbaum
    Associates.
-   Popham, W. J. (1981). Modern educational measurement. Englewood
    Cliffs, NJ: Prentice Hall.
-   Reid, J. B. (1970). Reliability assessment of observation data: A
    possible methodological problem. Child Development, 41, 1143-1150.
-   Saslow, C. A. (1982). Basic research methods. New York: Random
    House.
:::
