# How accurate are the behaviour analysts' attempts to measure experimental effects? \n

::: documentByline
Prepared by John Church, PhD, School of Educational Studies and Human
Development

University of Canterbury, Christchurch, New Zealand.
:::

::: {#parent-fieldname-text-0d645eaaa51443c0bd39c583ced1833a}
Behaviour analytic reports of within-subject experiments generally show
a high degree of awareness of the conditions which must be met before an
experiment is likely to yield an interpretable result. Behaviour
analysis methods texts (e.g. Cooper, Heron & Heward, 1987) routinely
make reference to the need for experimental designs in which only one
variable is manipulated at a time, in which treatment fidelity is
demonstrated, in which baseline and treatment phases operate for a
sufficient length of time, and in which direct replication is used to
assess the reliability of measures of treatment effects.

Behaviour analysis researchers assess whether or not there is a
correlation between their independent and dependent variables by
examining the changes in performance which occur from one phase to the
next (as the independent variable is introduced, or withdrawn, or its
value changed). When a change in the independent variable is followed by
substantial changes in performance, that is, changes which can be seen
by visual inspection, this is taken as evidence of a correlation between
the independent and dependent variables.

For behaviour analysts, the achievement and demonstration of
experimental control is an essential component of the research
enterprise and the task of achieving experimental control is greatly
simplified by virtue of the fact that a within-subject methodology is
employed. Because each subject serves as his or her own control,
inter-subject differences in pre-experimental learning history, ability,
achievement, motivation, and so on cannot function as extraneous
variables in this kind of experiment.

With the effects of differences in prior history removed by the use of a
within-subject methodology, the remaining extraneous variables which
need to be controlled are (a) intra-subject variables (such as illness,
fatigue or self-directed learning between sessions), (b) variations in
the administration of the experimental treatment and (c) variations in
the conditions experienced during the control or baseline condition. In
general terms, behaviour analysts attempt to control the effects of each
of these classes of extraneous variables simply by taking a reasonable
degree of care in the design and implementation of their experiments.

Because multiple observations are being made under each experimental and
control condition, the failure to achieve adequate levels of
experimental control is often revealed by a pattern of within-phase
variability in the data. "The fact that each subject\'s behavior is
repeatedly measured under each supposedly constant condition permits
behavioral variability to serve as a metric of the degree of control
attained" (Johnston & Pennypacker, 1993, p. 184). When behaviour, or
rate of learning, fluctuates from day to day within a phase, this often
signals that unintended changes are occurring with respect to at least
one of the non-experimental conditions which affect the learning outcome
which is of interest. One of the strengths of the within-subject
methodology is that, where such fluctuations occur, the experimenter may
take action to identify the factor which is causing the unexpected
variability and either redesign the experiment to measure its effects,
or else redesign the experimental procedures to provide better control
over the extraneous variable and its effects.

Behaviour analysts interpret unexpected variability in their data as a
state of affairs which should be responded to by an attempt to improve
the experimental procedures *not* by recruiting more subjects, or by
aggregating data, or by resorting to a statistical analysis of the data.
"Eliminating the effects of a variable statistically is not equivalent
to eliminating its effects on behavior by controlling the variable
directly" (Johnston & Pennypacker, 1993, p. 243). The effects of
uncontrolled variables "are not cancelled statistically. They are simply
buried so that their effects cannot be seen" (Sidman, 1960, p. 162).

Behaviour analysts argue that a measure of experimental effect must be
shown to be reliable in the same way that a measure of behaviour change
must be shown to be reliable. They argue that a measure of treatment
effect may be judged to be reliable if it can be *reproduced.* "There is
no way actually to know whether experimental findings are reliable
except through conducting *direct replications*, which are studies that
essentially duplicate conditions of the original investigation" (Poling,
Methot & LeSage, 1995, p. 28).

Direct replication involves repeating the experiment. Most of the
experimental designs used by behaviour analysts are, in fact,
replication designs. These replication designs take a variety of forms
including alternating conditions designs, reversal designs, multiple
baseline designs, changing criterion designs, and so on. All of these
experimental designs have one characteristic in common and that is that
the experimenter attempts to obtain more than a single measure of the
effect of a particular experimental treatment on the learning outcome
which is of interest. A detailed discussion of the replication designs
most commonly used in behaviour analysis research will be found in
Cooper, Heron & Heward (1987).

Generally speaking, the journals which publish behaviour analysis
experiments require a demonstration by the experimenter that his or her
experimental effects can be and have been replicated. As a result, the
great majority of the environment-behaviour relationships described in
behaviour analysis reports have been replicated many times. The fact
that behaviour analysts employ measurement procedures and experimental
procedures which are likely to generate accurate measurement results,
institute regular reliability and replicability checks, and must report
the results of these data quality evaluations as a condition of
publication represents a very considerable advance over the data quality
assurance procedures employed by both ethnographers and cognitive
scientists.
:::

::: referencesList
#### References

-   Cooper, J. O., Heron, T. E., & Heward, W. L. (1987). Applied
    behavior analysis. New York: Macmillan Publishing Co.
-   Johnston, J. M., & Pennypacker, H. S. (1993). Strategies and tactics
    of behavioral research (2nd ed.). Hillsdale, NJ: Lawrence Erlbaum
    Associates.
-   Poling, A., Methot, L. L., & LeSage, M. G. (1995). Fundamentals of
    behavior analytic research. New York: Plenum Press.
-   Sidman, M. (1960). Tactics of scientific research: Evaluating
    experimental data in psychology. New York: Basic Books.
:::
