---
layout: default
title: "Upgrading the quality of research funded by the Ministry of Education 
"
nav_order: 4
has_children: false
---
# Upgrading the quality of research funded by the Ministry of Education 


::: documentByline
Prepared by John Church, PhD, School of Educational Studies and Human
Development

University of Canterbury, Christchurch, New Zealand.
:::

::: {#parent-fieldname-text-a75a2a8bc42f4af486b1226ffd718562}
In New Zealand, the Ministry of Education is a the major source of funds
for research into all aspects of learning, teaching, materials
development and school functioning. During 2005 and 2006 more than 50
research reports were published by or for the Ministry of Education.
This suggests that the Ministry funds more than 25 new research projects
per year. Some of these projects are simply reports of statistics
collected by the Ministry. Included here are annual reports on the
enrolments in Reading Recovery, enrolments with Resource Teachers:
Literacy, attendance and absence statistics, and workforce statistics.
Some are reports of national and international studies of student
achievement. Included here are reports from the National Education
Monitoring Project and the Progress in International Reading Literacy
Study (PIRLS).

However, much Ministry funded research is research undertaken, under
contract, by non-Ministry researchers working on questions selected by
the Ministry. The way in which these research contracts are designed,
put out to tender, funded, and reported on provide very strong
incentives to the rest of the educational research community. It is
important, therefore, that these projects not only meet Ministry
objectives but also provide models of worthwhile research undertaken
using appropriate research methods.

Some of the research projects which are being commissioned by the
Ministry are conceptually and technically sophisticated and serve as
excellent models of the kind of research required for the transition to
evidence-based practice. Recent examples include the research reviews
which meet conventional quality standards (e.g. Church, 2003; Timperley,
Wilson, Barrar & Fung, 2007), the longitudinal Competent Learners Study
(Wylie, Ferral, Hodgen & Thompson, 2006), the International Reading
Studies (Comparative Education Research Unit, 2005), the National
Education Monitoring Project, and the Te Kötahitanga project (Bishop,
Berryman, Cavanagh & Teddy, 2007).

Other aspects of Ministry sponsored research leave much to be desired.
Ministry research contracts have been criticised for seeking answers to
questions which a brief commissioned project cannot provide, for
commissioning projects which are too small to answer the questions of
interest, and for setting RFP deadlines which are far too short to
permit organisation of the release time and substitute teaching
arrangements necessary in order for suitably qualified staff to mount
the requested project (Alcorn, 2005).

In order to gain some idea of the overall quality of Ministry of
Education research we examined many of the reports of contract research
funded by the Ministry over the period 2005-2006 together with Teaching
and Learning Research Initiative (TLRI) reports published over the
period 2004-2005 to get an idea of their quality. We looked at three
aspects of quality: the value and importance of the research question(s)
addressed, the adequacy of the research designs employed to answer these
questions, and the conceptual sophistication of the research (especially
the potential for the research to generate new knowledge).

**Importance of the questions asked**

An examination of Ministry of Education research reports over this
period quickly reveals that many of the questions which are being
studied are not research questions at all. Some of the questions are so
trivial as not to qualify as research questions, some of the questions
addressed are questions to which the answer is already known, and some
questions are so general that there is no research method which could
ever answer them. The twelve 2004-2005 TLRI reports tended to address
the most trivial questions. In all but one case, these studies are
descriptive studies which addressed trivial questions regarding existing
practices. The following examples illustrate the relatively
unsophisticated nature of the research questions addressed by both TLRI
funded and contract funded Ministry research.

Peterson and Irving (2004), using funds from the \$5m Teaching and
Learning Research Initiative, reported on their attempts to answer
several research questions relating to secondary school assessment
practices. One of these questions was "What is the impact of students'
conceptions of assessment and feedback on students' learning outcomes?"
This question is, of course, non-sensical because it is assessment
*practices* which affect student learning not students' *conceptions* of
those practices. A second question addressed by this study was "What
factors contribute to the success of the TLRI programme? This question
is also unanswerable because the answer depends upon the technical
sophistication of researchers and the authors of the study had no
intention of studying a sample of researchers who varied with respect to
technical sophistication.

Ellis, Loewen and Hacker (2005), under contract to the Ministry of
Education, attempted to evaluate the impact of additional funding under
the Second Language Learning Funding Pool. One of the main indictors was
"enrolments in second language classes". This is the kind of information
which can be collected by a short telephone survey and hardly warrants
an expensive research contract.

Waiti (2005), under contract to the Ministry of Education, undertook an
evaluation of the effects of Kaupapa Ara Whakawhiti Mätauranga, a set of
school-based ICT initiatives. One of the aims of the evaluation was to
examine relationships between the uses being made of the three ICT
tools, school curriculum organisation, school curriculum content, within
school teaching approaches, student learning, student interest, teacher
interest, teacher motivation, student retention, degree of professional
development, degree of inter-school collaboration, school management,
and community involvement. Even if a reliable measure could have been
generated for each of these variables (which wasn't the case), this
question is so complex that it almost certainly could never have been
answered in any meaningful manner and, of course, it wasn't.

Winter (2005), reported the results of a Ministry contract to evaluate
the effects of four computer enhanced after school study centres. One of
the aims of this project was to describe lessons learned from the pilot
that could usefully inform any roll out of the scheme to other schools
and other school districts. While this is an interesting question, it is
clearly too broad to function as a research question since there are
literally hundreds of possible answers to the question.

**The adequacy of the research designs used**

Of the external research projects under review, a dozen purport to be
evaluation studies in that they have the words *an evaluation of* in
their title. However, closer examination of the research method employed
in these studies indicates that none of them actually qualify as
evaluation studies because none of them employed an evaluation design.
In order to evaluate the effects of introducing a new programme, several
design criteria must be met. Where the aim is to see whether the new
programme has resulted in enhanced student learning there must be a
reliable measure or measures of student learning or achievement. Where
the aim is to measure the effects of introducing some kind of programme
or initiative, the sample of classrooms (or schools) must be of
sufficient size to generate generalizable conclusions. Where the aim is
to measure effects, then either (a) the outcomes must be measured both
prior to, as well as after, the introduction of the new programme or
else (b) half of the sample must be randomly assigned to a control or
placebo condition. These are the only ways of evaluating the effects of
introducing something new. *None of the 2005-2006 Ministry funded
"evaluations" meet these criteria.* The following examples illustrate
this observation.

In evaluating the effects of funding under the Second Language Learning
Funding Pool (Ellis et al., 2005) referred to above, two of the stated
aims of the project were to measure benefits with respect to student
learning and benefits with respect to teacher capability. However, no
attempt was made to measure either of these educational outcomes, so no
conclusions could be drawn about the effects of the extra funding on
either of these outcomes.

Mitchell, Tangaere, Mara, and Wylie (2006), completed an "evaluation" of
the effects of equity funding on levels of participation and service
quality in a sample of 47 early childhood education centres. However,
there was no control group nor were measures of any of the indicators
obtained before and after receipt of the equity funding, so no
conclusions could be drawn about the effects of the equity funding.

One of the aims of study by Winter (2005) of the effects of four
computer enhanced after school study centres was to identify the stated
learning objectives and the observed learning outcomes of these study
centres. However, the learning outcomes were never measured, there was
no control group, and no before and after observations were made. These
shortcomings meant that no conclusions whatsoever could be drawn
regarding the effects of this initiative on student learning outcomes.

Not only are completely inappropriate procedures being employed in
Ministry "evaluations", they are also being employed in a number of the
literature reviews commissioned by the Ministry. A considerable amount
of money has now been spent on a set of literature reviews within the
Best Evidence Synthesis programme of the Medium Term Strategy Division.
One of the essential elements of a review of research is a clearly
stated set of inclusion criteria. A clear description of the inclusion
criteria is absolutely essential to the success of the review activity
because (a) only if the inclusion criteria are clearly stated can the
reviewer determine when the literature search has been completed and (b)
clearly stated inclusion criteria are necessary in order for the reader
to determine whether the standards set for inclusion were sufficient to
allow conclusions to be drawn from the set of reports included in the
review. So far, only one of the five published BES reviews (Timperley,
Wilson, Barrar & Fung, 2007) meets these requirements.

One of the consequences of the failure to specify a set of inclusion
criteria is that each of the first three BES reviews has failed to
include large numbers of relevant research reports. For example, the BES
review of mathematics teaching (Anthony & Walshaw, 2007) failed to
review more than 250 controlled experimental analyses of mathematics
learning in school aged children and, on these grounds alone, is
virtually worthless as a review.

A second consequence of the failure to operate clearly specified
inclusion criteria is that many of the studies which have been included
in these reviews are quite irrelevant to the questions addressed by the
review. For example, the Alton-Lee (2003) review purports to address
questions relating to the effect of classroom variables and teaching
variables on children's learning outcomes. However, the review includes
many descriptive studies which are quite irrelevant to the questions
asked and these descriptive studies have been summarised and have been
used to generate conclusions *as if they were relevant* to the questions
being addressed. This practice is inexcusable in a Ministry funded
literature review.

**Conclusion**

Our analysis of Ministry procedures and projects indicates a strong
focus on descriptive studies of "what is" with large sums of scarce
research funding being spent on evaluations which are not evaluations,
literature reviews which are not literature reviews, and other research
projects which fail to meet even minimal quality standards.

The educational research budget in New Zealand simply is not large
enough to justify this kind of wastage. Worse still, this work sends
completely inappropriate signals to educational researchers. The
Ministry of Education commands a very significant proportion of the
research funding available for educational research in New Zealand. The
projects which it supports therefore function as models and standards
for educational researchers to emulate. These standards are currently
being set so low that Ministry research is currently subverting the
Ministry's stated aim of encouraging the move to evidence-based practice
in this country.
:::

::: referencesList
#### References

-   Alcorn, N. (2005). Will scholarship trump teachers in New Zealand
    teacher education? Waikato Journal of Education, 11, 3-16.
-   Alton-Lee, A. (2003). Quality teaching for diverse students in
    schooling: Best evidence synthesis. Wellington, New Zealand: New
    Zealand Ministry of Education.
-   Anthony, G., & Walshaw, M. (2007). Effective pedagogy in
    mathematics/pängarau. Palmerston North, N.Z.: Massey University,
    School of Curriculum and Pedagogy.
-   Bishop, R., Berryman, M., Cavanagh, T., & Teddy, L. (2007). Te
    Kötahitanga phase 3 whänaungatanga: Establishing a culturally
    responsive pedagogy of relations in mainstream secondary school
    classrooms. Report to the Ministry of Education. Hamilton, N.Z.:
    University of Waikato, School of Education in association with
    Poutama Pounamu Research and Development Centre.
-   Church, R. J. (2003). The definition, diagnosis and treatment of
    children and youth with severe behaviour difficulties: A review of
    research. Report prepared for the Ministry of Education.
    Christchurch, N.Z.: University of Canterbury, Education Department.
-   Comparative Educational Research Unit. (2005). Processes of reading
    comprehension: A summary of the results from the Progress in
    International Reading Literacy Study 2001. Wellington, N.Z. Ministry
    of Education.
-   Ellis, R., Loewen, S., & Hacker, P. (2005). Evaluation of the Second
    Language Learning Funding Pool (1999-2003). Auckland, N.Z.: The
    University of Auckland, Department of Applied Language Studies and
    Linguistics.
-   Mitchell, L., Tangaere, A. R., Mara, D., & Wylie, C. (2006). An
    evaluation of initial uses and impact of Equity Funding. Final
    report for Ministry of Education. Wellington, N.Z.: New Zealand
    Council for Educational Research.
-   Peterson, E., & Irving, E. (2004). Conceptions of assessment and
    feedback. Auckland, N.Z.: The University of Auckland, Department of
    Psychology.
-   Timperley, H., Wilson, A., Barrar, H., & Fung, I. (2007). Teacher
    professional learning and development. Best evidence synthesis
    iteration (BES). Wellington, New Zealand: New Zealand Ministry of
    Education.
-   Waiti, P. (2005). Evaluation of Kapapa Ara Whakawhiti Mätauranga.
    Wellington, N.Z.: New Zealand Council for Educational Research.
-   Winter, M. (2005). Digital Opportunities Pilot Project (2001-2003):
    Evaluation of digitally boosted study support centres. Report to the
    Ministry of Education. Christchurch, N.Z.: Ultralab South.
-   Wylie, C., Ferral, H., Hodgen, E., & Thompson, J. (2006).
    Competencies at age 14 and competency development for the Competent
    Children: Competent Learners study sample. Wellington, N. Z.: New
    Zealand Council for Educational Research.
:::
