---
grand_parent: 'How are the effects of teaching on learning to be measured? '
great_grand_parent: 'Types of research evidence '
great_great_grand_parent: 'Teacher Education''s Core Knowledge and Skills.'
has_children: false
layout: default
nav_order: 2
parent: 'How accurate are within-subject and between-group measures of experimental
  effects? '
title: 'How accurate are the cognitive scientists'' attempts to measure experimental
  effects? '
---
# How accurate are the cognitive scientists' attempts to measure experimental effects?


```yaml
Prepared by John Church, PhD, School of Educational Studies and Human
Development

University of Canterbury, Christchurch, New Zealand.
```


In a between-groups experiment the researcher determines whether or not
a correlation exists between the independent and dependent variables
selected for study by examining the size of the difference between the
mean performance of control group subjects and the mean performance of
experimental group subjects at the end of the experiment.

Social science methods texts almost always contain a discussion of the
internal validity of between-groups experiments, that is, the kinds of
extraneous variables which need to be controlled if the experimenter is
to be able to draw a valid conclusion from the results of such an
experiment. Discussions of the extraneous variables which pose the
greatest threat to the internal validity of a between-groups experiment
are usually adaptations of a discussion first published by Campbell and
Stanley in 1963. In their original article, Campbell and Stanley listed
the following threats to the internal validity of a between-groups
experiment: *history* (the operation of extraneous variables during the
course of the experiment), *maturation* (incidental learning by and
other uncontrolled changes in the subjects during the course of
experiment), *testing* (the effects of earlier tests on the scores on
later tests), *instrumentation* (the effects of unreliable testing or
observation procedures), *regression towards the mean* (changes which
occur when experimental groups are constituted on the basis of extreme
scores on a classificatory measure prior to the experiment), *selection*
(confounds caused by the selection of groups which are incomparable at
the outset), and *experimental mortality* (confounds caused by the
unequal loss of subjects from experimental and control groups).

Other factors which can make it difficult to interpret the results of a
between-groups experiment include failure of the random assignment
procedure to generate equivalent groups, failure to implement the
experimental treatments in the manner intended by the experimenter,
experimental treatments which operate for too short a period for their
effects to be revealed, failure to control relevant learning experiences
outside of the treatment periods, failure to control for teacher or
therapist commitment to one of the experimental treatments, failure to
avoid ceiling or floor effects on measures of learning outcomes, failure
to take into account the biasing effects of one or two greatly superior
(or greatly inferior) scores on the outcome measure, and so on
(Krathwohl, 1985; Lawson, 1997; Lysynchuk, Pressley, d\'Ailly, Smith &
Cake, 1989).

In order to achieve a level of experimental control sufficient to allow
conclusions to be drawn from the experimental results, investigators who
use the between-groups methodology are counselled to select outcome
measures which are demonstrably valid and reliable, to assign at least
30 subjects to each treatment group, to randomly assign subjects to
treatments, to operate treatments for a reasonable length of time, to
ensure that treatments are administered in the manner intended, to
provide adequate controls over extraneous variables, to use an
appropriate experimental design, and so on (see, for example, Campbell &
Stanley, 1963; Cook & Campbell, 1979; Krathwohl, 1985).

Most methods texts recognise that non-equivalent experimental groups
constitute one of the most serious threats to internal validity and
include a series of suggestions as to how this problem can be avoided.
These suggestions include: the random assignment of subjects to
experimental and control groups, matching cases (selecting pairs of
individuals with closely similar characteristics) and assigning one of
them to the experimental and the other to the control group, matching
groups (ensuring that the pre-experimental means and standard deviations
of the experimental and control groups on selected variables are closely
similar), or the use of analysis of covariance procedures as the
analytic technique (see, for example, Sax, 1968). Sometimes attempts are
made to control for the effects of variability in a particular
organismic variable by building that variable into the experiment as one
of the experimental factors.

Some social science methods texts also discuss the issue of experimental
sensitivity, the factors on which it depends, and the steps which can be
taken in order to ensure that between-groups experiments are
sufficiently sensitive to detect treatment effects (see, for example,
Lawson, 1997; Lipsey, 1990). Social scientists usually refer to
experimental sensitivity as the *power* of the experiment. Some of the
factors identified by Lawson (1997) as factors affecting power
(experimental sensitivity) include: the degree of variability between
subjects with respect to performance on the outcome measure, ambiguity
in the instructions to subjects, variability in the administration of
treatments across teachers or sites, the use of outcome measures which
lack sensitivity, the use of unreliable outcome measures, variability in
the administration of outcome measures, sample size, and the type of
inferential statistic selected.

One of the difficulties faced by the authors of between-groups
experiments is that the difference between the mean score of the
experimental group and the mean score of the control group at the end of
the experiment is often quite small relative to the spread of scores
within each group - making it difficult to ascertain whether the
experimental treatment has had an effect or not.

The most commonly used solution to this problem is the one first
suggested by R.A. Fisher (1935). Once the group means have been
calculated, social scientists usually apply a statistical significance
testing procedure to their data in order to decide whether or not any
conclusions should be drawn from that data. To decide whether any
conclusions should be drawn from a set of group means, between-groups
researchers ask the question "Is this difference greater than might be
expected on the basis of \'chance\', that is, on the basis of errors of
measurement alone?" The chance expectation is referred to as the *null
hypothesis*. The null hypothesis is a statement to the effect that
"these groups have all been drawn from the same population" or, more
specifically, that the differences between these means are no greater
than the differences which tend to occur when samples are drawn from a
single population. "Statistical hypotheses must be tested against
something. . . . The alternative usually selected is the null
hypothesis, which was invented by Sir Ronald Fisher. The *null
hypothesis* is a statistical proposition which states, essentially, that
there is no relation between the variables" (Kerlinger, 1964, p. 173-4).
If the experimental results fit the chance model, then they are said to
be *not significant*. If they depart sufficiently from the chance model,
then they are said to be statistically *significant*.

Differences between two or more mean scores are said to be statistically
significant if there is a less than 1 in 20 probability of them fitting
the chance (the random differences) model. The selection of the .05
confidence level for rejecting the null hypothesis is largely a matter
of custom although not entirely so. "A level of statistical significance
is to some extent chosen arbitrarily. But it is certainly not completely
arbitrary. . . . The .05 and .01 levels correspond fairly well to two
and three standard deviations from the mean of a normal probability
distribution. . . . The .05 level was originally chosen - and has
persisted with researchers - because it is considered a reasonably good
gamble. It is neither too high nor too low for most social scientific
research" (Kerlinger, 1964, p. 154).

The calculation of descriptive, inferential and probability statistics
has been greatly simplified during recent years with the appearance of
computer programmes designed specifically to compute such statistics.
However, the researcher who uses such programmes must still have a
knowledge of statistics sufficient to determine which statistical tests
can appropriately be applied to the data collected, sufficient to
determine whether or not the data meet the assumptions of that
statistical test, and sufficient to interpret the results of the
computer generated statistical analysis in the light of the experimental
procedures used.


#### References

-   Campbell, D. T., & Stanley, J. C. (1963). Experimental and
    quasi-experimental designs for research on teaching. In N. Gage
    (Ed.), Handbook of research on teaching (pp. 171-246). Chicago: Rand
    McNally.
-   Cook, T. D., & Campbell, D. T. (1979). Quasi-experimentation: Design
    & analysis issues for field settings. Chicago: Rand McNally
    Publishing Co.
-   Fisher, R. A. (1935). The design of experiments. Edinburgh,
    Scotland: Oliver and Boyd.
-   Kerlinger, F. N. (1964). Foundations of behavioral research:
    Educational and psychological inquiry. New York: Holt, Rinehart and
    Winston Inc.
-   Krathwohl, D. R. (1985). Social and behavioral science research: A
    new framework for conceptualizing, implementing and evaluating
    research studies. San Francisco: Jossey-Bass Publishers.
-   Lawson, M. J. (1997). Experimental studies. In J. P. Keeves (Ed.),
    Educational research, methodology, and measurement: An international
    handbook (2nd ed., pp 126-134). Oxford, England: Pergamon/Elsevier
    Science Inc.
-   Lipsey, M. (1990). Design sensitivity: Statistical power for
    experimental research. Newbury Park: Sage.
-   Lysynchuk, L., Pressley, M., d\'Ailly, H., Smith, M, & Cake, H.
    (1989). A methodological analysis of experimental studies of
    comprehension strategy instruction. Reading Research Quarterly, 24,
    458-470.
-   Sax, G. (1968). Empirical foundations of educational research.
    Englewood Cliffs, NJ: Prentice-Hall Inc.
