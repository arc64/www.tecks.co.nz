---
grand_parent: 'How are the effects of teaching on learning to be measured? '
great_grand_parent: 'Types of research evidence '
great_great_grand_parent: 'Teacher Education''s Core Knowledge and Skills.'
has_children: false
layout: default
nav_order: 3
parent: 'How accurate are within-subject and between-group measures of experimental
  effects? '
title: 'Problems arising from the inappropriate use of significance testing procedures '
---
# Problems arising from the inappropriate use of significance testing procedures


```yaml
Prepared by John Church, PhD, School of Educational Studies and Human
Development

University of Canterbury, Christchurch, New Zealand.
```


Social science research strategies such as the between-groups
experiment, and the statistical procedures used to interpret the data
from such experiments, have been the subject of critical scrutiny for
more than 40 years. This critical scrutiny has identified a number of
fatal shortcomings in these research strategies as they are
conventionally employed. These include problems arising from the
inappropriate use of significance testing procedures and problems
inherent in the hypothetico-deductive method used by social scientists.

Null hypothesis significance testing (NHST) has been referred to as a
"tyranny" (Loftus, 1991), as "the most misused and misconceived
hypothesis testing model employed in psychology" (Nunnally, 1960, p.
642), as "the most boneheadedly misguided procedure ever
institutionalised in the rote training of science students" (Rozeboom,
1997, p. 336), as "one of the worst things that ever happened in the
history of psychology" (Meehl, 1978, p. 817) and as a procedure which
"has not only failed to support the advance of psychology as a science
but also has seriously impeded it" (Cohen, 1997, p. 22).

All of the NHST procedures are procedures which seek an answer to the
following question: "What is the probability that these two (or more)
groups were drawn from the same population?" By convention, if this
probability is more than 1 in 20, the null hypothesis (that the groups
are from the same population) is accepted and, if this probability is
less than 1 in 20, the null hypothesis is rejected and the results are
labelled *statistically significant*.

Critics of the NHST procedure have pointed out that this procedure says
nothing about the effects of the experimental treatments, it says
nothing about the probable cause of differences between the mean
post-test scores of the experimental groups, and it says nothing about
whether the same result would be obtained if the experiment was
replicated.

The inferential errors which are most commonly made when applying the
NHST procedure are as follows.

The first mistake which researchers make is to argue that the complement
of the p value (e.g. 1 - .05 = .95) indicates the likelihood that the
research hypothesis is true (e.g. that a given teaching method did, in
fact, have an effect on retention). This inference cannot be made from
the results of a significance test (Carver, 1978; Cohen, 1997).

Too many people in education do not seem to realize that a statistically
significant result at the .05 level has nothing directly to do with
inferences about the research hypothesis. Even if the null hypothesis
can be rejected, several other alternative or rival hypotheses still
must be ruled out before the validity of the research hypothesis is
confirmed. Only after rigorous theorizing, careful design of
experiments, and multiple replications of the findings in varied
situations should one contend that the probability is high that the
research hypothesis is true (Carver, 1978, p. 386).

A second mistake which social science researchers routinely make is to
argue that the complement of a p value indicates the probability that a
given result will be replicated by future investigators. This inference
cannot be made from the results of a significance test.

Whether or not the results are replicable or reliable actually depends
upon whether the important variables can be controlled and manipulated
in exactly the same way to give the same results. . . . there is no
magic in the numbers collected and analyzed using the assumptions of the
null hypothesis that allows us to infer anything about the probability
that another researcher will be able to get a similar mean difference. .
. . Too often statistical significance is substituted for actual
replicative evidence; too often statistical significance covers up an
inferior research design (Carver, 1978, p. 385-386).

A number of social scientists (e.g. Robinson & Levin, 1997; Steiger,
1990; Thompson, 1999) have argued (as do behaviour analysts) that
demonstrating an effect to be reproducible is far more important than
demonstrating an effect to be statistically significant. "The only
conclusive evidence for result replicability is to actually replicate
the study. And replication studies are important, and are too
undervalued in the contemporary social sciences" (Thompson, 1999, p.
71).

To summarise, "the test of significance does not provide the information
regarding psychological phenomena characteristically attributed to it"
(Bakan, 1967, p. 1-2). "The mere rejection of a null hypothesis provides
only meager information" (Nunnally, 1960, p. 643). It is "singularly
unsuited to the task of advancing the development of cumulative
scientific knowledge" (Schmidt & Hunter, 1997, p. 56) and may well be
one of the main reasons why psychology has failed to develop as a
scientific endeavour (Meehl, 1978). What learning researchers and
teachers want to know is *how much of an effect* a particular type of
experience has on learning. "Physical scientists have learned much by
storing up amounts, not just directions" (Tukey, 1969). But randomised
groups experiments which rely on the NHST procedure provide us with no
information about this important question.

Critics of the NHST procedure argue that the valid interpretation of
research results is more dependent upon sound reasoning than it is upon
the calculation of any inferential statistic, that the *p* value
produced by a significance test is uninterpretable unless accompanied by
estimates of power and of effect size, that experimental hypotheses
should be quite specific about the size of the effect to be observed so
that if the risky hypothesis is confirmed it provides strong
corroboration for the theory, and that all studies should be replicated
several times with only the reproducible results being published (see,
for example, Harlow, 1997).


#### References

-   Bakan, D. (1967). On method: Toward a reconstruction of
    psychological investigation. San Francisco: Jossey-Bass.
-   Carver, R. P. (1978). The case against statistical significance
    testing. Harvard Educational Review, 48, 378-399.
-   Cohen J. (1997). The earth is round (p\<.05). In L. L. Harlow, S. A.
    Mulaik, & J. H. Steiger (Eds.), What if there were no significance
    tests? (pp. 21-35). Mahwah, N.J.: Lawrence Erlbaum Associates.
-   Harlow, L. L. (1997). Significance testing introduction and
    overview. In L.L. Harlow, S. A. Mulaik, & J.H. Steiger (Eds.), What
    if there were no significance tests? (pp. 1-17). Mahwah, N. J.:
    Lawrence Erlbaum Associates.
-   Loftus, G. R. (1991). On the tyranny of hypothesis testing in the
    social sciences. Contemporary Psychology, 36, 102-105.
-   Meehl, P. E. (1978). Theoretical risks and tabular asterisks: Sir
    Karl, Sir Ronald, and the slow progress of soft psychology. Journal
    of Consulting and Clinical Psychology, 46, 808-834
-   Nunnally, J. (1960). The place of statistics in psychology.
    Educational and Psychological Measurement, 20, 641-650.
-   Robinson, D., & Levin, J. (1997). Reflections on statistical and
    substantive significance, with a slice of replication. Educational
    Researcher, 26(5), 21-26.
-   Rozeboom, W. W. (1997). Good science is abductive, not
    hypothetico-deductive. In L. L. Harlow, S. A. Mulaik, & J. H.
    Steiger (Eds.), What if there were no significance tests? (pp.
    335-339). Mahwah, NJ: Lawrence Erlbaum Associates.
-   Schmidt, F. L. & Hunter, J. E. (1997). Eight common but false
    objections to the discontinuation of significance testing in the
    analysis of research data. In L. L. Harlow, S. A. Mulaik, & J. H.
    Steiger (Eds.), What if there were no significance tests? (pp.
    38-64). Mahwah, NJ: Lawrence Erlbaum Associates.
-   Steiger, J. H. (1990). Structural model evaluation and modification:
    An interval estimation approach. Multivariate Behavioral Research,
    25, 173-180.
-   Thompson, B. (1999). Five methodological errors in educational
    research: A pantheon of statistical significance and other faux pas.
    In B. Thompson (Ed.), Advances in social science methodology Vol. 5,
    (pp. 23-86). Stamford, CT: JAI Press Inc.
-   Tukey, J. W. (1969). Analyzing data: Sanctification or detective
    work? American Psychologist, 24, 83-91.
