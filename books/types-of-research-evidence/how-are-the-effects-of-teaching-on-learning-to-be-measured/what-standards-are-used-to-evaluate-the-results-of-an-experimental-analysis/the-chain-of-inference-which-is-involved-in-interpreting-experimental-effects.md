---
grand_parent: 'How are the effects of teaching on learning to be measured? '
great_grand_parent: 'Types of research evidence '
great_great_grand_parent: 'Teacher Education''s Core Knowledge and Skills.'
has_children: false
layout: default
nav_order: 4
parent: 'What standards are used to evaluate the results of an experimental analysis? '
title: 'The chain of inference which is involved in interpreting experimental effects '
---
# The chain of inference which is involved in interpreting experimental effects


```yaml
Prepared by John Church, PhD, School of Educational Studies and Human
Development

University of Canterbury, Christchurch, New Zealand.
```


As stated above, a relatively complex chain of reasoning is involved in
arriving at a valid interpretation of the results of an experiment.

First, the experimenter must be able to demonstrate that the various
treatments operated in the manner described in the report. This aspect
of experimental design is usually referred to as *treatment fidelity* or
*treatment integrity*. In the case of a within-subject experiment,
treatment fidelity can be assessed by observing and recording the
implementation of the experimental treatments as the experiment moves
from one experimental phase to the next. In the case of a between-groups
experiment, treatment fidelity can be assessed by carefully observing
and recording the learning opportunities experienced by each of the
experimental and control group subjects. The results of such
observations are commonly referred to as measures of *procedural
reliability* (Cooper, Heron & Heward, 1987). Only if the description of
the independent variable manipulation is believable, will attempts to
argue that learning was affected by that manipulation also be considered
believable.

Secondly, the experimenter must be able to demonstrate that measurably
different rates of learning occurred under each of the experimental
conditions. In a within-subject experiment this involves presenting data
which show, fairly unambiguously, that the introduction of the
experimental treatment was accompanied by a detectable change in
performance, competence, or rate of learning. In a between-groups
experiment this involves presenting data which show, fairly
unambiguously, that there was a detectable difference between the mean
post-test score of the experimental group and that of the control group.
With this kind of experiment it is often necessary to apply a
statistical test to the mean scores of the several experimental groups
in order to demonstrate that the differences between these means was
greater than might be expected on the basis of random performance
fluctuations attributable to differences in prior learning. The drawing
of valid inferences from such tests is sometimes referred to as
*statistical conclusion validity* (Krathwohl, 1985).

Thirdly, the experimenter must be able to argue, successfully, that "the
independent variable is the only thing that could possibly explain the
changes in the dependent variable measures that were observed when the
experimental condition was implemented and terminated" (Johnston &
Pennypacker, 1993, p. 241). This is variously referred to as
demonstrating *experimental control*, demonstrating *internal validity*,
or demonstrating *conclusion validity*. "Experiments that show
convincingly that changes in behavior are a function of the independent
variable and are not the result of uncontrolled or unknown variables are
said to have a high degree of *internal validity"* (Cooper, Heron &
Heward, 1987, p. 147).

Of course demonstrations of treatment fidelity, statistical conclusion
validity and internal validity, by themselves, do not guarantee that the
experimenter has obtained an accurate measure of treatment effects. In
order to determine whether a particular experimental procedure has
produced an accurate measure of treatment effects, the experimental
procedure must be administered on more than one occasion so that the
degree of similarity between the results of the two separate experiments
can be observed. (This is analogous to the test of *measurement*
reliability. In order to demonstrate that a particular *measurement* has
produced an accurate measure of learning one must administer that
measurement procedure on more than one occasion and observe a high
degree of similarity between the results of the two separate measurement
attempts.)

The repeated administration of a particular experiment, under closely
similar conditions, is referred to as *direct replication* (Sidman,
1960). The results of two or more direct replications of the same
experiment may be directly compared by the reader. When a set of direct
replications produces a set of experimental effects of closely similar
size, this provides evidence that the experimental procedures are ones
which produce an accurate measure of treatment effects.

The question of whether or not a particular experiment has been
replicated and, if so, whether the replication has produced a closely
similar effect, is absolutely critical when it comes to interpreting the
results of research into teaching and learning. Replication is far more
important than statistical conclusion validity because a successful
replication tells the reader that a particular effect is reproducible.
Knowing that there is a statistically significant difference between the
means of two sets of scores does not tell the reader than a given
experimental treatment has a reproducible effect. Social science
researchers have tended to argue that between-groups experiments do not
need to be replicated because they involve a large number of subjects.
However, the critical datum on which conclusions are based typically
consists of but a single mean score for the experimental group and a
single mean score for the control group and the reproducibility of a
mean score (just like the reproducibility of an individual score) can
only be determined by direct replication.


#### References

-   Cooper, J. O., Heron, T. E., & Heward, W. L. (1987). Applied
    behavior analysis. New York: Macmillan Publishing Co.
-   Johnston, J. M., & Pennypacker, H. S. (1993). Strategies and tactics
    of behavioral research (2nd ed.). Hillsdale, NJ: Lawrence Erlbaum
    Associates.
-   Krathwohl, D. R. (1985). Social and behavioral science research: A
    new framework for conceptualizing, implementing and evaluating
    research studies. San Francisco: Jossey-Bass Publishers.
-   Sidman, M. (1960). Tactics of scientific research: Evaluating
    experimental data in psychology. New York: Basic Books.
