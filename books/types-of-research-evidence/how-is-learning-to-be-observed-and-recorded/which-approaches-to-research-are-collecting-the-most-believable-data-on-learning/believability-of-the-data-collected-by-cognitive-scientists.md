---
grand_parent: 'How is learning to be observed and recorded? '
great_grand_parent: 'Types of research evidence '
great_great_grand_parent: 'Welcome to Teacher Education''s Core Knowledge and Skills.'
has_children: false
layout: default
nav_order: 2
parent: 'Which approaches to research are collecting the most believable data on learning? '
title: 'Believability of the data collected by cognitive scientists '
---
# Believability of the data collected by cognitive scientists


```yaml
Prepared by John Church, PhD, School of Educational Studies and Human
Development

University of Canterbury, Christchurch, New Zealand.
```


Researchers who work in the social science tradition consider data
quality issues to be matters of some importance. "Research findings
requiring the use of tests can be no more accurate than the measures
themselves" (Sax, 1968, p. 154). Hence most social science methods texts
devote at least a chapter to discussions of the reliability and validity
of tests and other measurement procedures. In cases where the researcher
has to devise their own test (rather than making use of an existing
test) the researcher is usually counselled that "much care should be
devoted to the objective evaluation of the instrument, and the findings
of the evaluation should be included in the research report" (Sax, 1968,
p. 156).

**Accuracy of the data collected**

Social science methods texts usually use the term reliability to refer
to consistency in the measurement result. "The problem of reliability is
essentially one of determining the degree of consistency present in any
set of observations or measurements (Sax, 1968, p. 157). Some methods
texts consider test reliability as an aspect of generalizability.
Thorndike and Thorndike (1997, p. 775), for example, argue that the
reliability question asks "how accurately the observed sample \[of
responses\] represents the broader universe of responses from which it
is drawn."

In classical test theory a person\'s score on a test is considered to
consist of two components the *true* score and a random *error*. "The
true score is defined as the value that the average of repeated
measurements with the identical measure approaches as the number of
measurements . . . is increased . . . The term \'identical\' implies
that it is possible to measure an individual repeatedly without changing
that individual - a condition which obviously cannot be achieved in the
real world" (Thorndike & Thorndike, 1997, p. 776). Allen & Yen (1979, p.
72) argue that "a test is reliable if its observed scores are highly
correlated with its true scores" but note that "in most cases true
scores cannot be obtained."

Most methods texts describe three general procedures for assessing the
reliability of a test. These are (a) the test-retest method (in which
the same test is administered to the same sample of subjects on two
separate occasions), (b) the parallel forms method (in which two
parallel forms of the same test are administered to the same sample of
subjects) and (c) the split half method in which two scores are obtained
for each subject (for example, the score on odd items and the score on
even numbered items). In all three cases, reliability is measured by
computing the correlation between the two sets of scores - as first
suggested by Spearman in 1904 (Allen & Yen, 1979; Thorndike & Thorndike,
1997).

The value of the test-retest and parallel forms reliability coefficients
depends not only on test reliability but also upon the range of ability
(range of scores) in the reliability sample (Thorndike & Thorndike,
1997). Hence these reliability coefficients can only be interpreted if
the standard deviation of the scores of the reliability sample is known.
With the split half method, the correlation coefficient is usually
adjusted to take into account the fact that reliability is being
calculated for a test which is only half as long as the actual test. The
split half-method is, perhaps, the most commonly used (Thorndike &
Thorndike, 1997). As already noted, however, the split-half method does
not actually provide a measure of reliability because the test is
administered only once.

For investigations which use previously standardised tests (such as
standardised achievement tests), test-retest reliabilities will usually
be known and can be used to judge the reliability of the data collected
by the investigator.

Social science methods texts tend to argue that inaccurate measurement
results (which social scientists refer to as errors of measurement) can
arise in any of three ways: (a) as a result of variability in the making
of the observation or judgement by the observer, (b) as a result of
variability in the results produced by the instrument being employed,
and (c) as a result of variability between learners with respect to the
characteristic (the construct) being measured, and further argue that
measurement errors are best managed by making multiple observations
(Keeves, 1997). Less common are discussions of the unreliability which
can arise as a result of inadequate sampling of potential test items, as
a result of variability in the way in which teachers and research
assistants follow the test administration and test scoring instructions,
and as a result of using a test which has been designed for ranking and
selection purposes as a measure of learning or achievement following
instruction.

**Validity of the conclusions which are being drawn from the data**

The instruments used by social science researchers almost always involve
a *sample* of the items or questions which might have been asked. Where
an instrument involves only a small sample of all possible items,
questions can always be asked as to whether or not the instrument
constitutes a valid measure of the construct under investigation. This
question, which is usually referred to as the validity question, asks
whether an instrument actually measures what it was supposed to measure.

Three types of test validity are described in most methods texts:
construct validity, criterion validity (often considered under the
headings of concurrent validity and predictive validity) and content
validity. Construct validity is usually assessed by computing the
correlation between the scores of a sample of subjects on the test which
is of interest and the scores of those same subjects on another test
which has been designed to measure the same construct. Criterion
validity is assessed by computing the correlation between the test
scores of a sample of subjects and some future criterion such as
examination success. Although content validity is extremely important in
any experiment designed to measure the effects of teaching on learning,
procedures for evaluating content validity have yet to be developed
(Zeller, 1997).

**Evaluation of the quality assurance procedures used by cognitive
scientists**

Learning involves a *transition* - from a state of not being able to do
something to a state of being able to do it. But researchers who have
applied the social science methodology to the study of learning have
devoted almost no effort to solving the problem of how such transitions
might best be observed and measured. "Psychologists are busy studying
the transition state called learning without being able to identify,
with any reasonable degree of precision, either the beginning or the end
of the process" (Sidman, 1960, p. 289).

Not only have cognitive scientists failed to develop an adequate
technology for studying the transitions which we refer to as learning,
they have not even developed adequate procedures for measuring the
achievement which results from a sequence of instruction. All too often
cognitive scientists simply make use of some kind of standardised
achievement test as the measure of learning outcomes. But standardised
achievement tests are designed to distinguish between pupils (e.g. for
selection purposes), they are not designed to measure the effects of
instruction. "We have a very bad, but deeply ingrained, habit of using
tests designed for the purpose of *discriminating among individuals*
when the evaluation of instruction should actually be the real concern"
(Skager & Weinberg, 1971, p. 35).

With standardised tests, item selection is rigged to produce a normal
distribution of scores. They are "quite insensitive to the effects of
short term educational experience" (Skager & Weinberg, 1971, p. 36).
This item selection procedure precludes the possibility that all the
test takers might obtain scores in the region of, say, 90-100%. When
measuring the achievement which results from a sequence of instruction,
however, items need to be selected to measure student learning of just
those understandings and skills which were the aim of instruction and
all items should be of similar difficulty. Under these circumstances a
"test on which a majority of students have near-perfect scores is not a
bad test at all; it in fact reflects the success of our efforts at
teaching" (Skager & Weinberg, 1971, p. 35). This problem too, has been
largely ignored. Teaching researchers who use the social science
methodology have developed no agreed procedures for checking the content
validity of tests which are being used to measure the effects of a
particular sequence of instruction.

Of course not all investigations used standardised tests of known
reliability. Many use tests, rating scales, observation schedules or
self-report instruments which have been specifically designed for that
investigation. In most investigations, these instruments are
administered only once. This means that the cognitive scientist cannot
demonstrate the reliability of that instrument during the course of the
investigation. In theory, the reliability of the instruments could be
established prior to the investigation using a test-retest procedure -
but this is only rarely done. In most investigations, the scores of
individual learners on each of the instruments is not reported and the
accuracy of those individual scores is neither assessed nor reported.
This routine failure to consider data quality issues (in spite of the
fact that procedures have been developed for assessing data quality) is
a major weakness of much of the published cognitive science research.


#### References

-   Allen, M. J., & Yen, W. M. (1979). Introduction to measurement
    theory. Belmont, CA: Wadsworth.
-   Keeves, J. P. (1997). Introduction: Advances in measurement in
    education. In J. P. Keeves (Ed.), Educational research, methodology,
    and measurement: An international handbook (2nd ed., pp. 705-712).
    Oxford, England: Pergamon/Elsevier Science Inc.
-   Sax, G. (1968). Empirical foundations of educational research.
    Englewood Cliffs, NJ: Prentice-Hall Inc.
-   Sidman, M. (1960). Tactics of scientific research: Evaluating
    experimental data in psychology. New York: Basic Books.
-   Skager, R. W. & Weinberg, C. (1971). Fundamentals of educational
    research: An introductory approach. Glenview, IL: Scott, Foresman &
    Co.
-   Thorndike, R. L., & Thorndike, R. M. (1997). Reliability. In J. P.
    Keeves (Ed.), Educational research, methodology, and measurement: An
    international handbook (2nd ed., pp. 775-798). Oxford, England:
    Pergamon/Elsevier Science Inc.
-   Zeller, R. A. (1997). Validity. In J. P. Keeves (Ed.), Educational
    research, methodology, and measurement: An international handbook
    (2nd ed., pp. 822-829). Oxford, England: Pergamon/Elsevier Science
    Inc.
